* Evaluation Function
** Problem
  - gets stuck around bad scores: ≈ -13—-10
    - worst score would be ≈-18 
    - really bad scores might correspond to *good* programs!
  - does not reflect program semantics well
    - most close programs *do not* differ by a small number of bits
    - is actively counter-productive for things like bitwise not
    - can find relatively odd almost-correct programs
** Benchmarks
  - try finding better function from existing benchmarks
    - some sort of (non?)linear regression?
    - I don't think this would solve the fundamental issues
  - let programmer specify it somehow?
  - do more random cases help smooth out the evaluation function?
    - fails to address the fundamental issue
    - more cases would only be good for catching weird edge cases
  - have some way of evaluating the evaluation function (very meta)
    - a graph would probably help
    - surprise: turns out they're uniformly horrible
** Uniqueness
  - come up with a bunch of metrics and then look for ones that stand
    out rather than having a preconceived notion of "goodness"
    - this really needs fleshing out and I haven't explained it very well
** Random Thoughts
  - look at program traces instead of results
    - trace inherently carry more information
    - especially for Forth, the results are far too volatile
    - maybe look at Δtraces—something like how states change rather than
      the states themselve
  - philosophy: the distance function depends more on the programs
    than their outputs
    - rather than trying to see how close two outputs are, in some
      sense (like popCount or arithmetic), estimate how far a program
      to generate them would be
    - maybe mix popCount and so on with one or two layers of
      transformations based on instructions? That is: take the answer
      and apply a bunch of random operations to it like bitwise
      negation and shifting, and average the popCount metric over
      those
   - hedge: try to account for weird shapes in the evaluation function
     rather than assuming any particular pattern ahead of time.
* Prior Distribution
  - how can we come up with a better distribution of mutations?
    - look at existing code
      - find common patterns
      - build up model of code: something like a Markov chain?
      - there simply isn't enough existing code to make this
        worthwhile
  - immediate problem: multi-instruction sequences
    - having a noop like b! !b in the candidate stalls the search
    - should be easy to fix by tweaking the jump distribution
* Test Cases
  - randomly generate initial cases
    - fix things like unspecified parts of the stack
  - use something like CEGIS
    - would need a rebuilt verifier—maybe with sbv (I hope)?
    - write my own SKETCH frontend, based on sbv?
* Numbers
  - relatively good at generating programs, especially with certain
    constraints (large memory usage, high bit precision,
    self-modifying, branching, looping...)
  - relatively *bad* at finding good numeric constants
    - could be helped by better distribution over constants: I
      currently just have a uniform distribution, which is indubitably
      horrible. 
    - however: perhaps it is better to use an intelligent solver for
      just this? That is: MCMC generates a program with a constant,
      but without specifying the *value* of the constant; when running
      the test, we could use a different solver to find a good value
      for the constant (if possible)
      - perhaps this is too slow? maybe a very specific solver/search
        algorithm would help here?
